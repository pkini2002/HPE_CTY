{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Packages**"
      ],
      "metadata": {
        "id": "wpWeg_qWnfuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpuFFXJEnRFw"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "## (C)Copyright 2021,2022 Hewlett Packard Enterprise Development LP\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
        "## not use this file except in compliance with the License. You may obtain\n",
        "## a copy of the License at\n",
        "##\n",
        "##    http://www.apache.org/licenses/LICENSE-2.0\n",
        "##\n",
        "## Unless required by applicable law or agreed to in writing, software\n",
        "## distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
        "## WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
        "## License for the specific language governing permissions and limitations\n",
        "## under the License.\n",
        "############################################################################\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "#from swarmlearning.tf import SwarmCallback Use Jupyter Notebook On Linux After Installing Wheel File"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Dataset Related Function and Parameters:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lIQAKQPxn1kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function TO Split Features and Labels Of A Dataset\n",
        "def getXY(dataSet):\n",
        "    np.random.shuffle(dataSet)\n",
        "    length = np.size(dataSet,0)\n",
        "    X = dataSet[0:length, :-1]\n",
        "    y = dataSet[0:length, -1:]\n",
        "    return X , y\n",
        "\n",
        "\n",
        "# Constant Dataset Names for Training and Testing\n",
        "testFileName = 'SB19_CCFDUBL_BAL_TEST_2C.csv'\n",
        "fileNameList = [\n",
        "    'SB19_CCFDUBL_BAL_TRAIN_2C.csv'\n",
        "  , 'SB19_CCFDUBL_BAL_P1_2C.csv'\n",
        "  , 'SB19_CCFDUBL_BAL_P2_2C.csv'\n",
        "  , 'SB19_CCFDUBL_BAL_P3_2C.csv'\n",
        "]\n",
        "\n",
        "#Setting Batch Size and Epoch Parameters\n",
        "part = 0\n",
        "batchSize = 32\n",
        "defaultMaxEpoch = 1000\n",
        "defaultMinPeers = 2"
      ],
      "metadata": {
        "id": "SZu0dCGDn2Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Model Training Function:**\n",
        "\n",
        "Workflow Of Main Function:\n",
        "1.   Set Paths for the dataDir,modelDir and scratchDir\n",
        "2.   Load Training and Test Data\n",
        "3.   Define the Model to be used to Train the Data\n",
        "4.   Add the SwarmCallback from swarmlearning.tf\n",
        "5.   Train the Model On Training Data\n",
        "6.   Evaluate The Model Based On Testing Data\n",
        "7.   Save the Model In the Path Mentioned\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2nIgvnonqrRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  modelName = 'fraud-detection' #Change Value To Change Filename of New Model\n",
        "  dataDir = os.getenv('DATA_DIR', '/platform/data') # Change to Path of Dataset\n",
        "  modelDir = os.getenv('MODEL_DIR', '/platform/model') # Change to Path of Model\n",
        "  scratchDir = os.getenv('SCRATCH_DIR', '/platform/scratch')  #Change to Path of Directory where New Model is Saved\n",
        "  maxEpoch = int(os.getenv('MAX_EPOCHS', str(defaultMaxEpoch))) #Set maxEpochs \n",
        "  minPeers = int(os.getenv('MIN_PEERS', str(defaultMinPeers))) #Set minPeers\n",
        "\n",
        "  print('***** Starting model =', modelName)\n",
        "\n",
        "\n",
        "  # ================== Loading Test and Train Data =========================\n",
        "  print('-' * 64)\n",
        "  fname = fileNameList[part]\n",
        "  trainFile = dataDir + '/' + fname\n",
        "  print(\"Loading Train Dataset %s ..\" % trainFile)\n",
        "  with open(trainFile, 'r') as f:\n",
        "    # Removing Header From Train Data\n",
        "    trainData = np.array(list(csv.reader(f, delimiter=\",\"))[1:], dtype=float)\n",
        "    print('size of training Data set : %s' % np.size(trainData,0))\n",
        "\n",
        "  print('-' * 64)\n",
        "  testFile = dataDir + '/' + testFileName\n",
        "  print(\"Loading Test Dataset %s ..\" % testFile)\n",
        "  with open(testFile, 'r') as f:\n",
        "    # Removing Header From Test Data\n",
        "    testData = np.array(list(csv.reader(f, delimiter=\",\"))[1:], dtype=float)\n",
        "\n",
        "  print('-' * 64)\n",
        "\n",
        "\n",
        "  # ================== Define Model to Train and Evaluate =========================\n",
        "  # Logistic Regression Model\n",
        "  model = tf.keras.models.Sequential() #Defining TF Model\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(1, input_shape=(30,), activation='sigmoid',\n",
        "    kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
        "  sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "\n",
        "  print('Starting training ...')\n",
        "  x_train, y_train = getXY(trainData)\n",
        "  x_test, y_test = getXY(testData)\n",
        "\n",
        "\n",
        "\n",
        "  # ================== Adding Swarm Callback =========================\n",
        "  swarmCallback = SwarmCallback(syncFrequency=128, minPeers=minPeers)\n",
        "\n",
        "  \n",
        "\n",
        "  # ================== Training the Model with Callback on Train Dataset =========================\n",
        "  model.fit(\n",
        "      x_train\n",
        "    , y_train\n",
        "    , batch_size=batchSize\n",
        "    , epochs=maxEpoch\n",
        "    , validation_data=(x_test, y_test)\n",
        "    , shuffle=True\n",
        "    , callbacks=[swarmCallback]\n",
        "  )\n",
        "\n",
        "  print('Training done!')\n",
        "\n",
        "    # ================== Evaluate the Model on Test Dataset =========================\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('***** Test loss:', scores[0])\n",
        "  print('***** Test accuracy:', scores[1])\n",
        "\n",
        "\n",
        "  # ================== Save the Model in the Scratch Directory =========================\n",
        "  os.makedirs(scratchDir, exist_ok=True)\n",
        "  model_path = os.path.join(scratchDir, modelName)\n",
        "  model.save(model_path)\n",
        "  print('Saved the trained model!')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ljRhaQcEpjfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}